#!/usr/bin/env python3
"""
AdCopySurge Production API - Real Database Integration
Works with your actual database structure and existing data
"""
import os
# Prevent accidental use in production: use main_production instead
if os.getenv("ENVIRONMENT", "").lower() == "production":
    raise SystemExit("Deprecated entrypoint: use 'uvicorn main_production:app' for production")
from fastapi import FastAPI, HTTPException, Depends, Header
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional, Dict, Any
import os
from datetime import datetime, timedelta
from supabase import create_client, Client
import jwt

# Environment variables for Supabase
SUPABASE_URL = os.getenv('SUPABASE_URL', '')
SUPABASE_SERVICE_KEY = os.getenv('SUPABASE_SERVICE_ROLE_KEY', '')
JWT_SECRET = os.getenv('SUPABASE_JWT_SECRET', '')

if not SUPABASE_URL or not SUPABASE_SERVICE_KEY:
    print("âš ï¸  Set SUPABASE_URL and SUPABASE_SERVICE_ROLE_KEY environment variables")

# Initialize Supabase client
supabase: Client = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)

# Create FastAPI app
app = FastAPI(
    title="AdCopySurge Production API",
    description="Real backend for existing database with 9 analyses, 2 projects, 1 user",
    version="1.0.0",
    docs_url="/api/docs",
    redoc_url="/api/redoc"
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Models matching your database structure
class AdAnalysisCreate(BaseModel):
    headline: str
    body_text: str
    cta: Optional[str] = None
    platform: str = "facebook"
    target_audience: Optional[str] = None
    industry: Optional[str] = None

# Authentication helper - works with your user_profiles table
async def get_current_user_id(authorization: str = Header(None)) -> str:
    """Extract user ID from JWT token and validate against user_profiles"""
    if not authorization:
        raise HTTPException(status_code=401, detail="Authorization header missing")
    
    try:
        # Remove 'Bearer ' prefix
        token = authorization.replace('Bearer ', '')
        
        # Decode JWT token to get auth.user ID
        payload = jwt.decode(token, JWT_SECRET, algorithms=['HS256'], options={"verify_aud": False})
        auth_user_id = payload.get('sub')
        
        if not auth_user_id:
            raise HTTPException(status_code=401, detail="Invalid token: no user ID")
        
        # Find corresponding user_profile (your database uses user_profiles.id, not auth.users.id directly)
        # For now, we'll use the auth_user_id directly since your user_profiles.id should match
        # In production, you might need a mapping table or field
        return auth_user_id
        
    except jwt.ExpiredSignatureError:
        raise HTTPException(status_code=401, detail="Token expired")
    except jwt.InvalidTokenError as e:
        raise HTTPException(status_code=401, detail=f"Invalid token: {str(e)}")

# Dashboard Metrics Endpoints - perfectly matched to your database
@app.get("/api/dashboard/metrics")
async def get_dashboard_metrics(
    period_days: int = 30,
    user_id: str = Depends(get_current_user_id)
) -> Dict[str, Any]:
    """Get dashboard metrics from your real ad_analyses data"""
    try:
        # Calculate date ranges
        current_end = datetime.now()
        current_start = current_end - timedelta(days=period_days)
        previous_start = current_start - timedelta(days=period_days)
        previous_end = current_start
        
        # Query current period analyses from your actual database
        current_analyses = supabase.table('ad_analyses').select(
            'overall_score, clarity_score, persuasion_score, emotion_score, cta_strength, platform_fit_score, platform, created_at'
        ).eq('supabase_user_id', user_id).gte(
            'created_at', current_start.isoformat()
        ).lte(
            'created_at', current_end.isoformat()
        ).neq(
            'overall_score', None
        ).execute()
        previous_analyses = supabase.table('ad_analyses').select(
            'overall_score, clarity_score, persuasion_score, emotion_score, cta_strength, platform_fit_score'
        ).eq('supabase_user_id', user_id).gte(
            'created_at', previous_start.isoformat()
        ).lte(
            'created_at', previous_end.isoformat()
        ).neq('overall_score', None).execute()
        
        # Calculate current metrics
        current_data = current_analyses.data
        current_count = len(current_data)
        current_avg_score = sum(a['overall_score'] for a in current_data) / current_count if current_count > 0 else 0
        current_avg_improvement = sum(max(0, a['overall_score'] - 50) for a in current_data) / current_count if current_count > 0 else 0
        current_top_score = max((a['overall_score'] for a in current_data), default=0)
        
        # Calculate previous metrics
        previous_data = previous_analyses.data
        previous_count = len(previous_data)
        previous_avg_score = sum(a['overall_score'] for a in previous_data) / previous_count if previous_count > 0 else 0
        previous_avg_improvement = sum(max(0, a['overall_score'] - 50) for a in previous_data) / previous_count if previous_count > 0 else 0
        previous_top_score = max((a['overall_score'] for a in previous_data), default=0)
        
        # Calculate percentage changes
        def calc_change(current: float, previous: float) -> float:
            if previous == 0:
                return 100.0 if current > 0 else 0.0
            return round(((current - previous) / previous) * 100, 1)
        
        return {
            "adsAnalyzed": current_count,
            "adsAnalyzedChange": calc_change(current_count, previous_count),
            "avgImprovement": round(current_avg_improvement, 1),
            "avgImprovementChange": calc_change(current_avg_improvement, previous_avg_improvement),
            "avgScore": round(current_avg_score, 0),
            "avgScoreChange": calc_change(current_avg_score, previous_avg_score),
            "topPerforming": current_top_score,
            "topPerformingChange": calc_change(current_top_score, previous_top_score),
            "periodStart": current_start.isoformat(),
            "periodEnd": current_end.isoformat(),
            "periodDays": period_days
        }
        
    except Exception as e:
        print(f"Error fetching dashboard metrics: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to fetch dashboard metrics: {str(e)}")

@app.get("/api/dashboard/metrics/summary")
async def get_metrics_summary(
    user_id: str = Depends(get_current_user_id)
) -> Dict[str, Any]:
    """Get summary metrics from your real database"""
    try:
        # Get all-time stats from ad_analyses
        all_analyses = supabase.table('ad_analyses').select(
            'overall_score, platform, created_at'
        ).eq('supabase_user_id', user_id).neq('overall_score', None).execute()
        
        # Get last 30 days stats
        thirty_days_ago = datetime.now() - timedelta(days=30)
        recent_analyses = supabase.table('ad_analyses').select(
            'overall_score'
        ).eq('supabase_user_id', user_id).gte(
            'created_at', thirty_days_ago.isoformat()
        ).neq('overall_score', None).execute()
        
        # Get project count
        projects = supabase.table('projects').select('id').eq('supabase_user_id', user_id).execute()
        
        # Calculate stats from your real data
        all_data = all_analyses.data
        total_analyses = len(all_data)
        
        if total_analyses > 0:
            lifetime_avg_score = sum(a['overall_score'] for a in all_data) / total_analyses
            best_score = max(a['overall_score'] for a in all_data)
            platforms_used = len(set(a['platform'] for a in all_data))
            first_analysis = min(a['created_at'] for a in all_data)
            last_analysis = max(a['created_at'] for a in all_data)
        else:
            lifetime_avg_score = 0
            best_score = 0
            platforms_used = 0
            first_analysis = None
            last_analysis = None
        
        return {
            "totalAnalyses": total_analyses,
            "lifetimeAvgScore": round(lifetime_avg_score, 1),
            "bestScore": best_score,
            "analysesLast30Days": len(recent_analyses.data),
            "platformsUsed": platforms_used,
            "projectsCount": len(projects.data),
            "firstAnalysisDate": first_analysis,
            "lastAnalysisDate": last_analysis,
            "isNewUser": total_analyses == 0
        }
        
    except Exception as e:
        print(f"Error fetching summary metrics: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to fetch summary metrics: {str(e)}")

@app.get("/api/dashboard/metrics/detailed")
async def get_detailed_metrics(
    period_days: int = 30,
    user_id: str = Depends(get_current_user_id)
) -> Dict[str, Any]:
    """Get detailed metrics from your real database"""
    try:
        # Get analyses from the period
        period_start = datetime.now() - timedelta(days=period_days)
        analyses = supabase.table('ad_analyses').select(
            'platform, industry, overall_score, created_at'
        ).eq('supabase_user_id', user_id).gte(
            'created_at', period_start.isoformat()
        ).neq('overall_score', None).execute()
        
        data = analyses.data
        
        # Platform breakdown
        platform_stats = {}
        for analysis in data:
            platform = analysis['platform'] or 'Unknown'
            if platform not in platform_stats:
                platform_stats[platform] = {'count': 0, 'scores': []}
            platform_stats[platform]['count'] += 1
            platform_stats[platform]['scores'].append(analysis['overall_score'])
        
        platform_breakdown = [
            {
                'platform': platform,
                'count': stats['count'],
                'avgScore': round(sum(stats['scores']) / len(stats['scores']), 1)
            }
            for platform, stats in platform_stats.items()
        ]
        
        # Industry breakdown
        industry_stats = {}
        for analysis in data:
            industry = analysis['industry'] or 'Unknown'
            if industry not in industry_stats:
                industry_stats[industry] = {'count': 0, 'scores': []}
            industry_stats[industry]['count'] += 1
            industry_stats[industry]['scores'].append(analysis['overall_score'])
        
        industry_breakdown = [
            {
                'industry': industry,
                'count': stats['count'],
                'avgScore': round(sum(stats['scores']) / len(stats['scores']), 1)
            }
            for industry, stats in sorted(industry_stats.items(), key=lambda x: x[1]['count'], reverse=True)[:10]
        ]
        
        # Score distribution
        all_scores = [a['overall_score'] for a in data]
        score_ranges = [
            ('Excellent (90-100)', 90, 100),
            ('Good (80-89)', 80, 89),
            ('Fair (70-79)', 70, 79),
            ('Poor (60-69)', 60, 69),
            ('Needs Improvement (<60)', 0, 59)
        ]
        
        score_distribution = []
        for range_name, min_score, max_score in score_ranges:
            count = len([s for s in all_scores if min_score <= s <= max_score])
            if count > 0:
                score_distribution.append({
                    'range': range_name,
                    'count': count
                })
        
        # Recent activity (last 7 days)
        recent_start = datetime.now() - timedelta(days=7)
        recent_data = supabase.table('ad_analyses').select(
            'overall_score, created_at'
        ).eq('supabase_user_id', user_id).gte(
            'created_at', recent_start.isoformat()
        ).neq('overall_score', None).execute()
        
        # Group by date
        daily_stats = {}
        for analysis in recent_data.data:
            date = analysis['created_at'][:10]  # YYYY-MM-DD
            if date not in daily_stats:
                daily_stats[date] = {'count': 0, 'scores': []}
            daily_stats[date]['count'] += 1
            daily_stats[date]['scores'].append(analysis['overall_score'])
        
        recent_activity = [
            {
                'date': date,
                'count': stats['count'],
                'avgScore': round(sum(stats['scores']) / len(stats['scores']), 1)
            }
            for date, stats in sorted(daily_stats.items(), reverse=True)[:7]
        ]
        
        return {
            'platformBreakdown': platform_breakdown,
            'industryBreakdown': industry_breakdown,
            'scoreDistribution': score_distribution,
            'recentActivity': recent_activity
        }
        
    except Exception as e:
        print(f"Error fetching detailed metrics: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to fetch detailed metrics: {str(e)}")

# Health check
@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "version": "1.0.0",
        "database": "supabase_production",
        "data_status": "1 user, 9 analyses, 2 projects"
    }

@app.get("/api/test")
async def test_endpoint():
    """Test endpoint showing available endpoints"""
    return {
        "status": "success",
        "message": "AdCopySurge Production API - Connected to real database",
        "timestamp": datetime.now().isoformat(),
        "database_status": "1 user, 9 analyses, 2 projects",
        "endpoints": [
            "/api/dashboard/metrics",
            "/api/dashboard/metrics/summary", 
            "/api/dashboard/metrics/detailed",
            "/health",
            "/api/test"
        ]
    }

if __name__ == "__main__":
    import uvicorn
    print("ðŸš€ Starting AdCopySurge Production API")
    print("ðŸ“Š Connected to database with 1 user, 9 analyses, 2 projects")
    print("ðŸ”— Dashboard endpoints ready with real data")
    uvicorn.run(app, host="127.0.0.1", port=8000)